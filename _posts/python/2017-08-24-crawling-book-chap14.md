---
layout: post
section-type: post
title: crawling - P2. 고급 스크레이핑 _ chap 14. 원격 스크레이핑
category: python
tags: [ 'python' ]
---

이번 챕터에서는 스크립트를 다른 컴퓨터, 또는 자신의 컴퓨터에서 다른 IP 주소를 사용해 실행하는 방법을 다룹니다.

## 14.1 원격 서버를 쓰는 이유

원격 서버는 웹 앱을 공개할 때 필수적이지만, 자신의 필요에 의해 만든 프로그램은 보통 로컬에서 실행됩니다. 원격 플랫폼을 사용하는 사람들이 그런 결정을 내리는 동기는 보통 두 가지입니다. 하나는 더 강력하고 유연한 환경이 필요해서이고, 다른 하나는 대체 IP 주소가 필요해서 입니다.

### 14.1.1 IP 주소 차단 방지

웹 스크레이퍼를 만들다 보면 무엇이든 속일 수 있다는 것을 알게 됩니다. 발신자 주소를 조작하여 이메일을 보낼 수도 있고, CLI에서 마우스를 움직일 수 있으며, 심지어 인터넷 익스플로러 5.0으로 사이트를 방문한 것처럼 속여서 사이트 관리를 멘붕으로 만들 수도 있습니다.  

단 하나 속일 수 없는 것은 IP 주소입니다.
> 기술적으로는 IP 주소도 속일 수 있습니다. 나가는 패킷을 위조하면 됩니다. 이는 분산형 서비스 거부 공격(DDOS)에서 사용하는 방법이며, 이걸 쓰면 공격자는 반환 패킷을 받지 못합니다.(받을 주소를 위조했으니까요) 하지만 웹스크레이퍼는 웹 서버의 응답을 바라고 하는 것이기 때문에 IP 주소는 속일 수 없는 것이라 할 수 있습니다.

스크레이퍼가 웹사이트에 접근하지 못하게 막는 노력의 대부분은 사람과 봇을 구별하는 것에 집중합니다. 봇을 막으려고 IP 주고까지 차단하는 건, 마치 농부가 농약 대신 밭을 불 태워버리는 것과 같습니다. 마지막 수단이긴 하지만, 말썽부리는 IP 주소에서 보내는 패킷을 모두 무시하는게 효율적인 수단인 것은 분명합니다. 하지만 이 방법에는 문제가 있습니다.

- IP 주소 접근 리스트를 관리하는건 매우 귀찮은 일입니다. 대형 웹사이트는 대부분 이런 리스트를 자동적으로 관리하는 프로그램을 사용하고 있지만(봇이 봇을 차단하네요...), 누군가는 가끔이라도 체크하거나 최소한 관찰이라도 하고 있어야 합니다.
- 서버는 패킷을 받을 때마다 그 패킷을 무시할지 받아들일지 리스트를 보고 판단해야 하므로, 각 주소에서 패킷을 받을 때마다 미미하지만 자원을 소비합니다. 주소와 패킷의 수를 곱하면 이 시간은 순식간에 늘어납니다. 처리 시간과 복잡성을 줄이기 위해, 공격자들이 밀집되어 있을 때 관리자들은 이들 IP 주소를 그룹으로 묶고 '이 범위에 있는 주소 256개를 모두 차단한다'같은 규칙을 만들곤 합니다. 여기서 세 번째 문제가 발생합니다.
- IP 주소 차단은 의도하지 않은 결과를 낳을 수 있습니다. IP 주소는 개인일 수도 있지만 공유기를 통해 전송된다면 IP 하나에 여럿 혹은 더 많은 사용자가 묶여 있을 수 있습니다. IP 하나를 차단했지만, 그로 인해 무고한 사용자(혹은 고객)까지 접속을 금지시킬 수 있습니다.

결점이 있음에도 불구하고, IP 주소 차단은 서버 관리자가 웹 스크레이퍼로 의심되는 무엇인가가 서버에 접근하지 못하게 할 때 많이 쓰이는 방법입니다.

### 14.1.2 이동성과 확장성

어떤 작업은 집에 있는 컴퓨터와 인터넷만으로 수행하기엔 너무 규모가 클 수도 있습니다. 웹사이트 하나에 그렇게 큰 부담을 싶지 않겠지만, 광범위한 사이트를 대상으로 데이터를 수집하다 보면 연결 대역폭과 저장 공간을 감당할 수 없을 때도 있습니다.

프로세서 자원을 많이 소모하는 작업을 다른 컴퓨터에 맡기면 집의 컴퓨터로는 더 중요한 작업을 할 수 있습니다. 컴퓨터의 전원이나 인터넷 연결에 신경 쓸 필요도 없습니다.  

애플리케이션이 컴퓨터 자원을 어마어마하게 소모하게 되어 아마존 임대 서버 하나로는 감당 할 수 없다면 **분산형 컴퓨팅** 을 알아보면 됩니다. 분산형 컴퓨팅은 여러 컴퓨터를 병렬로 연결해 한가지 작업을 수행하는 것입니다. 예를 들면, 컴퓨터 한 대가 한 사이트 집합에서 데이터를 수집하는 동안 다른 컴퓨터는 다른 사이트 집합에서 데이터를 수집하고, 두 컴퓨터는 수집된 데이터를 같은 데이터베이스에 저장하는 겁니다.  

누구든 구글 검색을 따라 할 수는 있지만, 구글 검색의 규모를 그대로 따라 할 수 있는 곳은 거의 없습니다. 분산형 컴퓨팅은 매우 방대한 분야입니다. 하지만 원격 서버에서 애플리케이션을 실행하는 것을 필수적인 첫 단계입니다.

## 14.2 토르

**토르** 로 더 잘 알려진 어니언 라우터(The Onion Router) 네트워크는 자발적으로 참여하는 서버로 구성된 네트워크입니다. 트래픽은 여러 계층으로 구성된 서버들을 거치며(이러한 점 때문에 양파입니다.) 어디서 출발했는지 알 수 없게 됩니다. 데이터는 네트워크에 들어 가기 전에 암호화되므로 특정 서버가 중간에서 감청되더라도 내용을 알 수 없습니다. 또한, 특정 서버에서 들어오고 나가는 통신을 탈취하더라도 통신의 시작과 끝을 정확히 파악하려면 그 통신 경로에 있는 **모든 서버** 의 들어오고 나가는 통신에 대해 자세히 알아야 하는데, 이건 거의 불가능한 일입니다.  

> ### 토르 익명성의 한계
여기서 토르를 사용하는 이유는 IP 주소를 바꾸는 것이지, 완벽한 익명성을 보장하는 것이 아닙니다. 토르의 트래픽 익명화에 대해 강점과 한계를 알아 보길 권장합니다.  
토르를 사용하면 IP 주소를 추적당하지 않긴 하지만, 웹 서버와 공유한 정보 자체에 당신을 노출할 단서가 있을 수 있습니다. 예를 들어 지메일 계정으로 로그인한 다음 구글 검색을 하면, 그 검색에서 당신의 정체가 드러날 수 있습니다.  
토르에 로그인하는 것만으로도 위협이 있을 수 있습니다. 2013년 12월, 하버드 학부생이 기말고사를 회피할 목적으로 토르 네트워크를 통해 익명 이메일 계정으로 대학을 공격한 일이 있었습니다. 하버드 IP 팀은 로그를 조사하여 그 공격이 이루어진 시간 동안 토르 네트워크에서 나간 트래픽은 단 하나의 컴퓨터에서 출발했다는 사실을 밝혀냈고 그 컴퓨터의 사용자로 등록된 학생을 알아냈습니다. IP 팀은 이 트래픽이 토르를 경유했다는 사실만 알아냈을 뿐 정확한 목적지를 찾아낼 수는 없었지만, 시간이 일치했고 그 시점에서 토르에 로그인 된 컴퓨터는 단 한 대였다는 사실만으로도 그 학생을 기소하기에는 충분했습니다.  
토르에 로그인한다고 해서 투명망토를 입는 것이 아니고, 인터넷에서 원하는 대로 할 수 있는 것도 아닙니다. 토르가 유용한 도구임은 분명하지만, 사용할 때는 충분히 책임감을 가지고 사용해야 합니다.

파이썬에서 토르를 사용하려면 따로 설치하고 실행해야 합니다. [토르 다운로드 페이지](https://www.torproject.org/download/download){:target="`_`blank"}에서 내려받아 설치하고, 실행해서 연결하기만 하면 됩니다. 토르를 사용하는 동안 인터넷 속도가 느려질 수도 있습니다. 패킷이 전 세계를 여러 번 왔다 갔다 하고 있을 수도 있기 때문입니다.

### 14.2.1 파이삭스

파이삭스(PySocks)는 프록시 서버로 트래픽을 돌리는 매우 단순한 파이썬 모듈이며 토르와 함께 잘 동작합니다.

##### 설치

```
pip install pysocks
```

파이삭스 문서는 자세히 나오지는 않지만 매우 단순합니다. 이 코드를 실행하려면 토르 서비스가 반드시 포트 9150(기본포트)에서 실행 중이어야 합니다.

```python
import socks
import socket
from urllib.request import urlopen

socks.set_default_proxy(socks.SOCKS5, "localhost", 9150)
socket.socket = socks.socksocket
print(urlopen('http://icanhazip.com').read())
```
http://icanhazip.com 사이트는 서버에 연결된 클라이언트의 IP 주소만 표시하며 테스트에 유용하게 쓸 수 있습니다. 이 스크립트를 실행한 결과로 표시된 IP 주소는 자신의 원래 IP와 달라야 합니다.  

```
b'192.160.102.169\n'
```

셀레니움과 펜텀JS를 써서 토르를 사용한다면 파이삭스가 필요하지 않습니다. 토르가 실행 중 일 때 셀레니움에서 포트 9150을 사용하게 하는 `service_args` 매개변수를 사용하기만 하면 됩니다.

```python
from selenium import webdriver
service_args = [ '--proxy=localhost:9150', '--proxy-type=socks5', ]
driver = webdriver.PhantomJS(executable_path='[path to PhantomJS]', service_args=service_args)
driver.get("http://icanhazip.com")
print(driver.page_source)
driver.close()
```

이번에도 표시된 IP 주소는 원래 IP가 아니라 토르가 현재 사용 중인 IP 주소여야 합니다.

```
<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">2605:e200:d00c:c01d::7777
</pre></body></html>
```

## 14.3 원격 호스팅

원격 호스팅을 사용하면 익명성은 사라지지만, 웹 스크레이퍼의 속도는 극적으로 빨라질 수 있습니다. 우리의 컴퓨터보다 서버 컴퓨터가 더 빠르기도 하지만, 목적지에 연결하는 과정에서 토르 네트워크 계층을 오가지 않기 때문입니다.

### 14.3.1  웹사이트 호스트 계정에서 실행

웹사이트를 가지고 있다면 외부 서버에서 웹 스크레이퍼를 실행 할 수단은 이미 가지고 있는겁니다. 명령줄에 접근할 수 없는 비교적 완고한 웹 서버에서도 웹 인터페이스를 이용하면 스크립트를 실행할 수 있습니다.  

리눅스 서버에서 웹사이트를 운영하고 있다면 파이썬은 이미 설치되어 있을 겁니다. 윈도우 서버라면 파이썬 설치를 확인하고, 안되어 있다면 서버 관리자에서 설치를 요청해야 합니다.  

웹 호스팅은 대개 **cPanel** 이라는 소프트웨어를 함께 제공합니다. 이 소프트웨어는 기본적인 관리 서비스와 함께 웹사이트 및 그와 관련된 서비스에 관한 정보를 제공합니다. cPanel에 접근할 수 있다면 아파치 핸들러에서 파이썬이 서버에 설치되어 있는지 알 수 있고, 파이썬 핸들러가 없다면 추가할 수 있습니다.

```
Handler: cgi-script
Extension(s): .py
```
이 명령은 모든 파이썬 스크립트를 CGI 스크립트를 통해 실행하라고 지시합니다. CGI란 **공통 게이트웨이 인터페이스(Common Gateway Interface)** 의 약자입니다. CGI는 서버에서 실행되는 프로그램이며 웹사이트에 표시될 콘텐츠를 동적으로 생성합니다. 파이썬 스크립트가 CGI 스크립트라고 명시적으로 정의하면 서버는 파이썬 스크립트를 브라우저로 바로 보내지 않고 실행 결과를 보냅니다.  

파이썬 스크립트를 작성해 서버에 올리고, 파일 퍼미션을 755로 바꿔 실행할 수 있게 만듭니다. 스크립트를 실행하려면 브라우저에서 그 위치로 이동하기만 하면 됩니다. 누구나 스크립트에 접근해서 실행하는 것을 막으려면 두 가지 방법이 있습니다.

- 스크립트를 저장할 때 짐작하기 어려운 URL을 사용하고, 다른 접근할 수 있는 URL에 스크립트를 가리키는 링크를 저장하지 않으면 검색 엔진이 스크립트를 인덱싱하지 못하게 막을 수 있습니다.
- 스크립트에 비밀번호를 지정하거나, 실행 전에 비밀번호 또는 비밀 토큰을 요구하게 만드십시오.

물론 웹사이트를 표시하도록 설계된 서비스에 파이썬 스크립트를 실행하는 것은 일종의 속임수입니다. 웹 스크레이퍼 웹 사이트는 로딩 시간이 더 깁니다. 사실 그 '페이지'는 스크랩이 완료되지 전에는 나타나지 않습니다. 스크레이퍼를 어떻게 만들었느냐에 따라 몇 분, 몇 시간이 걸릴 수도 있고 아예 끝나지 않을 수도 있습니다. 작업 자체는 확실히 끝나겠지만, 결과를 좀 더 실시간으로 확인하고 싶을 겁니다. 실제 서버가 필요한 건 이 때문입니다.

### 14.3.2 클라우드에서 실행

물리적인 컴퓨터 한 대에 대한 사용료를 내는 것이 아니라 처리 능력을 기준으로 비용을 지불해야 합니다. 작업 자체는 보통 여러 컴퓨터에서 나눠 실행합니다. 이 시스템의 모호한 구조 때문에 사용료는 요구치가 가장 높은 부분을 기준으로 산정됩니다. 예를 들어 아마존 스팟 인스턴스는 시간보다는 비용 절약에 더 관심이 있는 사람들을 위한 옵션입니다.  

서버 임대 옵션도 더 세분화되어, 애플리케이션의 필요에 따라 메모리 우선, 처리 속도 우선, 저장 공간 우선 등의 옵션을 선택할 수 있습니다. 웹 스크레이퍼는 보통 메모리를 크게 요구하지는 않으므로 범용적인 옵션보다는 처리 속도나 저장 공간을 우선시해서 선택하는 것이 좋을 수 있습니다. 자연어 처리를 많이 하거나, 이미지 인식을 많이 하거나, 위키백과의 여섯 다리 같은 경로 탐색 작업이 필요하다면 처리 속도에 중점을 두어야 합니다. 데이터를 굉장히 많이 스크랩하거나, 파일을 많이 저장하거나, 대규모 분석이 필요하다면 저장 공간에 중점을 두어야 합니다.  

클라우드 컴퓨팅 업계에는 셀 수 없이 많은 작은 회사들이 있고, 아마존과 구글이 패권을 다투고 있기 때문에 서버 인스턴스 설정은 점점 더 쉬워져서 이제는 앱 이름을 생각해뒀다가 몇 번 클릭하고 신용카드 번호를 입력하면 끝납니다. 처음 사용하는 사용자라면 아마존과 구글은 무료 체험도 가능합니다.(아마존은 처음 1년동안 프리티어를 준다.)  

서버 인스턴스를 설정하기만 하면 새 소유자 IP 주소와 사용자 이름, SSH 연결에 사용할 개인/공용 키를 받게 됩니다. 거기서부터는 직접 소유한 서버와 모든 것이 똑같습니다. 하드웨어를 직접 관리하거나, 모니터링 도구를 만들 필요도 없습니다.

## 14.4 추가 자료

최근에는 클라우드 컴퓨팅이 유행하고 회사들이 경쟁하면서 관련 도구도 극적으로 발전했습니다.  

하지만 대규모 스크레이퍼나 더 복잡한 스크레이퍼를 만들기 위해서는 데이터를 수집하고 저장하는 플랫폼에 대해 좀 더 알아야 합니다.  

< Google Compute Engine > 은 구글 클라우드 컴퓨팅에서 파이썬과 자바스크립트를 사용하는 법에 관한 가이드를 제공합니다. 구굴 사용자 인터페이스 뿐 아니라 애플리케이션을 유연하게 만들 수 있는 명령줄과 스크립트 도구도 설명합니다.

아마존을 선호한다면 <파이썬을 이용한 `AWS` 가이드> 를 권합니다. 아마존 웹 서비스의 기초와 함께 확장성 있는 애플리케이션 제작을 설명합니다.

## 14.5 미래를 향해

웹은 끊임없이 변하고 있습니다. 이미지, 비디오, 텍스트, 기타 데이터 파일을 전달하는 기술은 끊임없이 업데이트되고 새로 개발되고 있습니다. 이 조류를 따라가려면 인터넷에서 데이터를 스크랩하는 기술들도 반드시 함께 변해야 합니다.  

나중에는 자바스크립트는 거의 쓰이지 않고, HTML8 홀로그램을 분석해야 할 수도 있습니다. 하지만 웹 사이트를 성공적으로 스크랩하기 위해 필요한 사고방식과 일번적인 접근법은 바뀌지 않을 겁니다. 지금부터 먼 미래에 이르기까지, 웹 스크레이핑 프로젝트에 임할 때는 항상 스스로 질문을 해야합니다.

- 지금 내가 처한 문제는 무엇인가?
- 어떤 데이터가 문제 해결에 도움이 되고, 어디에서 그걸 찾을 수 있나?
- 웹사이트는 이 데이터를 어떻게 표시하나? 웹사이트의 코드 어느 부분에 이 정보가 들어 있는지 정확히 파악할 수 있나?
- 데이터를 어떻게 정확히 찾아서 가져올 수 있을까?
- 데이터를 더 유용하게 만들려면 어떻게 처리하고 분석해야 할까?
- 이 방식을 더 좋게, 더 빠르게, 더 견고하게 개선하려면 어떻게 해야 할까?

소개 했던 도구 하나하나를 이해하는데 그치지 말고 그들을 결합해 더 큰 문제를 해결하는데 쓸 수 있어야 합니다. 데이터를 쉽게 얻을 수 있고 형식도 더 정확해서 간단한 스크레이퍼로 일을 해결할 수 있을 때도 있을 겁니다. 생각을 많이 해야 할 때도 있을 겁니다.  

예를 들어 [챕터10](https://kimdoky.github.io/python/2017/08/13/crawling-book-chap10.html){:target="`_`blank"}에서는 셀레니움 라이브러리를 써서 아마존의 Ajax 이미지를 가져오고, 테서랙트로 그 이미지에서 글자를 인식했습니다. 위키백과의 여섯다리 문제에서는 정규 표현식을 써서 링크 정보를 데이터베이스에 저장하는 크롤러를 만들고, 그래프 해석 알고리즘으로 '케빈 베이컨과 에릭 아이들을 연결하는 가장 짧은 링크 경로는 무엇인가?'라는 문제를 풀었습니다.  

인터넷에서 자동으로 데이터를 수집하는 것에 관한 한 풀 수 없는 문제는 거의 없습니다. 인터넷은 사용자 인터페이스가 좀 부실한 거대한 API입니다.

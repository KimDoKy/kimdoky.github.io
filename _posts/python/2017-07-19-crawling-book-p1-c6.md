---
layout: post
section-type: post
title: crawling - P1.스크레이퍼 제작 _ cahp 6. 문서 읽기
category: python
tags: [ 'python' ]
---

이번 챕터에서는 문서를 다루는 방법을 다룹니다. 로컬 폴더에 내려받거나 직접 읽고 데이터를 추출하는 것 모두 해당합니다. 다양한 텍스트 인코딩에 대해 다루고, 외국어로 된 HTML 페이지도 읽을 수 있게 됩니다.

## 6.1 문서 인코딩

문서 인코딩은 야플리케이션이 그 문서를 읽는 방법을 지정합니다. 인코딩은 보통 파일 확장자에서 추론할 수 있지만, 파일 확장자가 꼭 인코딩에 따라 정해지는 것은 아닙니다. 에를 들어 myImage.jpg를 myImage.txt로 저장해도 아무 문제가 없습니다. 최소한 텍스트 에디터로 그 파일을 열어보기전까지는요. 다행히 그런 상황은 드물고, 문서의 파일 확장자만 알아도 보통은 정확히 읽을 수 있습니다.  

모든 문서는 0과 1으로 인코딩되어 있습니다. 인코딩 알고리즘은 그 위에서 글자 하나에 몇 비트인지, 이미지 파일이라면 각 픽셀에 몇 비트를 써서 색깔을 나타내는지 정의합니다. 그 위에 다시 PNG 파일처럼 압축 같은 공간 절약 알고리즘이 있을 수 있습니다.  

알맞은 라이브러리를 쓰기만 하면 파이썬은 어떤 형식의 정보라도 제대로 다룰 수 있습니다. 텍스트 파일과 비디오, 이미지 파일의 차이는 0과 1을 어떻게 해석하느냐일 뿐입니다. 이번 챕터에서는 자주 접하는 TXT, CSV, PDF, DOCX에 대해 다룹니다.

## 6.2 텍스트

요즘 온라인에서 평범한 텍스츠 파일을 만나는 일은 드물지만, 서비스에 중점을 두지 않는 사이트나 오래된 형식을 따르는 사이트들은 아직 텍스트 파일을 대향으로 저장하고 있는 곳도 많이 있습닏. 예를 들어 국제 인터넷기술위원회(IETF)는 밥ㄹ행한 문서를 모두 HTML, PDF, TXT 파일로 저장하고 있습니다. (ex. https://ietf.org/rfc/rfc1149.tct) 대부분의 브라우저는 이들 텍스트 파일을 잘 표시하고, 스크랩하는 데도 아무런 문제가 없습니다.  

대부분의 기본적인 텍스트 문서, http://www.pythonscraping.com/pages/warandpeace/chapter1.txt 의 연습용 파일 같은 경우에는 다음 방법을 씁니다.

```python
from urllib.request import urlopen
textPage = urlopen("http://www.scraping.com/pages/warandpeace/chapter1.txt")
print(textPage.read())
```

일반적으로 urlopen으로 페이지를 가져오면 BeautifulSoup 객체로 바꿔서 HTML로 파싱합니다. 여기에서는 페이지를 직접 읽을 수 있습니다. BeautifulSoup 객체로 바꾸는 건 당연히 가능하지만, 파싱할 HTML이 없으므로 효율적이지 않고, 따라서 이 라이브러리를 여기서는 필요 없습니다. 일단 텍스트 파일을 문자열로 읽으면 파이썬에서 다른 문자열을 다루듯 분석할 수 있습니다. 물론 여기엔 단서로 사용할 HTML 태그가 없으므로, 필요한 텍스트와 쓸모없는 텍스트를 구분하기가 쉽지 않습니다. 이런 점이 텍스트 파일에서 정보를 추출할 때 부딪히는 여려운 문제입니다.

### 6.2.1 텍스트 인코딩과 인터넷

파일 확장자만 알면 파일을 정확히 읽을 수 있습니다만, 그 선언은 모든 문서 중에서 가장 기본적인 `.txt`파일에는 적용되지 않습니다.  

대부분은 위으 코드로 텍스트를 읽는데 아무 문제가 없지만, 인터넷의 텍스트에는 함정이 있습니다.  

다음 섹션에서는 영어와 외국어의 인코딩 기본인 ASCII와 유니코드, ISO에 대해 다룹니다.

#### 인코딩 타입 개관

1990년대 초반, 유니코드 컨소시엄이라는 비영리 제단에서 어떤 언어에서도, 어떤 텍스트 문자에서도 사용할 수 있고 모든 글자를 표현할 수 있는 인코딩을 만들려는 시도를 했습니다. 목표는 라틴 알파벳, 키릴문자, 한자, 수학고 논리기호, 이모티콘, 방사능 경고 등 모든 기호를 표현하는 것이었습니다.  
그 결과로 만들어진 인코딩은 UTF-8(Universal Character Set Transformation Format - 8-bit) 입니다. 8비트는 글자를 표시하기 위해 필요한 최소한의 크이입니다. 모든 글자를 8비트로만 저장하면 2^8, 즉 256개밖에 되지 않기 때문에 한자를 포함한 모든 글자를 담는 것은 불가능합니다.  

UTF-8의 각 글자는 '이 글자를 표현하는 데는 1바이트만 사용한다' 또는 '다음 2바이트가 한글자를 나타낸다'는 표시를 시작하며, 최대 4바이트까지 사용합니다. 한글자를 구성하는데 몇 바이트를 사용하는지 지정하는 정보는 각 바이트마다 들어 있으므로 32비트 전체를 사용하지는 못합니다. 실제로 사용하는 것은 21비트이며 총 2,097,152글자를 표현할 수 있고, 현재는 이 중에 1,114,112 글자를 사용하고 있습니다.  

여러 애플리케이션 입장에서 유니코드는 마치 하늘이 내려준 선물과도 같지만, 습관을 버리기 어렵다 보니 여전히 ASCII를 선호하는 사람도 많습니다.  

ASCII는 1960년대부터 사용된 텍스트 인코딩 표준입니다. 각 글자에 7비트를 사용하므로 총 2^7 = 128글자를 쓸 수 있습니다. 이 숫자는 알파벳 대소문자, 구두점 등 일반적인 영어권 사용자의 키보드에 있는 글자를 모두 나타내기에 충분합니다.  

1960년대에는 저장 비용이 상당히 고가였기 때문에 텍스트 파일을 저장할 때 글자당 7비트를 사용하는지 아니면 8비트를 사용하는지가 매우 중요했습니다. 당시의 컴퓨터 과학자들은 1비트를 추가해서 어림수(round number)를 쓰는 편리함과, 7비트만 써서 파일 크기를 줄이는 실용성을 놓고 대립하기도 했지만, 결국 7비트가 승리했습니다. 하지만 최근에는 각 7 비트 앞에 추가로 0을 붙이므로(추가한 비트를 '패딩 비트'라고 합니다.) 당시 논쟁의 나쁜 점만 취한 상황이 됐습니다.(파일은 14% 커졌지만 사용 가능 글자는 128개 뿐이니까요)  

UTF-8을 설계한 사람들은 이 '패딩 비트'를 활용하기로 결정하였습니다. 즉 0으로 시작하는 모든 바이트는 그 바이트 단 하나로 한 글자를 나타내게 만들어, ASCII과 UTF-8의 인코딩 스키마가 완전히 같게 한 겁니다. 다라서 다음 글자들은 UTF-8과 ASCII에서 모두 유효합니다.  

```
0100 0001 - A
0100 0010 - B
0100 0011 - C
```
그리고 다음 글자들은 UTF-8에서만 유효하며, 문서를 ASCII 문서로 해석할 떄는 표현할 수 없습니다.

```
1100 0011 1000 0000 - Å
1100 0011 1001 1111 - ß
1100 0011 1010 0111 - ç
```

UTF 표준에는 UTF-8 외에도 UTF-16,  UTF-24, UTF-32 같은 표준이 있지만, 이들 형식으로 인코드된 문서는 특정 상황이 아니면 거의 만나기 어렵습니다.  

모든 유니코드 표준에 공통인 문제는 ASCII 코드를 벗어나는 문자를 사용하는 문서가 필요 이상 커진다는 겁니다. 사용하는 언어가 100글자 내외만 사용한다 하더라도 각 글자를 나타내는데 최소한 16비트가 필요합니다. 따라서 영어가 아닌 텍스트 문서는 영어로 된 텍스트 문서에 비해 거의 두 배 크기가 됩니다.  

ISO는 각 언어에 특화된 인코딩을 만들어 이 문제를 해결하려 했습니다. ISO 인코딩은 ASCII 같은 인코딩 방식을 택하면서, 모든 글자의 첫 비트를 '패딩 비트'로 만들어 해당 언어에필요한 특수문자에 쓰려 했습니다. 이 방식은 라틴 알파벳(인코딩에서 0~127번)을 많이 사용하고 몇 가지 특수문자가 추가되는 유렵권 언어에는 잘 어울렸습니다. 이에 따라 라틴 알파벳을 위해 디자인 된 ISO-8859-1은 분수나 저작권 기호 같은 특수문자를 쓸 수 있게 됐습니다.  

다른 ISO 문자셋, 예를 들어 ISO-8859-9 (터키어)나 ISO-8859-2 (독일어), ISO-8859-15 (프랑스어) 등도 어느 정도 널리 쓰입니다.  

ISO 인코딩을 사용하는 문서는 최근 줄어들고 있지만, 웹사이트의 9% 정도는 여전히 ISO 인코딩을 사용하고 있습니다. 따라서 이런 내용을 미리 알고, 사이트 스크랩을 시작하기 전에 미리 인코딩을 체크하는 것이 중요합니다.

### 인코딩 예제

이전 섹션에서 urlopen의 기본 설정을 이용해서 텍스트 문서를 열었습니다. 이 방법은 대부분의 영어 텍스트에서는 잘 동작하지만, 러시아어나 아라비아어 등을 만나는 순간 문제가 발생합니다.  

```python
from urllib.request import urlopen
textPage = urlopen("http://pythonscraping.com/pages/warandpeace/chapter1-ru.txt")
print(textPage.read())
```

이 코드는 러시아어와 프랑스어로 쓰인 <전쟁과 평화> 원문의 1장을 읽어 화면에 출력합니다.

```
b"\xd0\xa7\xd0\x90\xd0\xa1\xd0\xa2\xd0\xac \xd0\x9f\xd0\x95\xd0\xa0\xd0\x92\xd0\x90\xd0\xaf\n\nI\n\n\xe2\x80\x94 Eh bien, mon prince.
```
또한 대부분의 브라우저에서 이 페이지를 방문하면 이상항 화면이 보입니다.

![]({{site.url}}/img/post/python/crawling/p1c6_2.png)

러시아어를 모국어로 사용하는 사람조차 이해하기 힘들 겁니다. 문제는 파이썬이 이 문서를 ASCII 문서로 읽으려 했고, 브라우저는 ISO-8859-1 문서로 읽으려 했습니다. 둘 중 어느 하나도 이것을 UTF-8 문서로 인식하지 못했습니다.  

이 문자열이 UTF-8이라고 명시적으로 지정하면 저확히 키릴 문자를 출력할 수 있습니다.

```python
from urllib.request import urlopen
textPage = urlopen("http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt")
print(str(textPage.read(), 'utf-8'))
```
이 개념을 BeautifulSoup와 파이썬 3.x에 적용된 코드는 다음과 같습니다.

```python
html = urlopen("http://en.wikipedia.org/wiki/Python_(programming_language)")
bsObj = BeautifulSoup(html, "html.parser")
content = bsObj.find("div", {"id":"mw-content-text"}).get_text()
content = bytes(content, "UTF-8")
content = content.decode("UTF-8")
```

앞으로 모든 웹 스크레이퍼에서 UTF-8 인코딩을 사용하고 싶어질 겁니다. UTF-8은 ASCII 글자도 문제없이 처리하기 때문입니다. 하지만 웹사이트의 9%가 ISO 버전 중 일부로 인코딩되어 있음을 기억해야 합니다. 텍스트 문서가 어떤 인코딩을 가졌는지 정확하게 판단하는건 불가능합니다. '특수한 문자'나 단어가 아닐 것이다 라는 로직을 써서 문서를 검사하고 인코딩을 짐작하는 라이브러리가 몇 가지 있지만 틀릴 때가 많습니다.  

다행히 HTML 페이지의 인코딩은 보통 <head> 내부의 태그에 들어 있습니다. 대부분의 사이트, 특히 영어로 된 사이트에는 다음과 같은 태그가 들어 있습니다.

```html
<meta charset="utf-8">
```
ECMA 인터내셔널 웹사이트(http://www.ecma-international.org/)에는 이런 태그가 있습니다.
```html
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
```
웹 스크레이핑을 많이 할 계획이고 특히 국제적 사이트에 관심이 있다면, 이 메타 태그를 찾아보고 이 태그에서 지정한 인코딩 방법을 써서 페이지 콘텐츠를 읽는게 좋습니다.

## 6.3 CSV

### 6.3.1 CSV 파일 읽기

## 6.4 PDF

## 6.5 마이크로소프트 워드와 .docx
